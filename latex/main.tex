\documentclass{ctexart}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\geometry{bottom=2cm} 
\ctexset { section = { format={\Large \bfseries } } }
\bibliographystyle{unsrt}

\begin{document}
\input{content/cover.tex}

\tableofcontents
\newpage

\section{引言}

\subsection{图嵌入的背景和意义}

众所周知，图是一种基础且常用的数据结构，它使用点和边的集合描述不同实体之间的关系。图广泛地存在于各种现实应用中，可以描述复杂的现实场景，如社交网络、交通网络、通信网络等，甚至生物中的蛋白质相互作用、一个句子都可以使用图结构进行描述。通过对图的分析，我们可以深入了解社会结构、语言和不同的交流模式，因此图一直是学界研究的热点。

使用图结构可以解决很多实际中的问题，如：社交网络中的关系预测、通信网络中异常节点的预测和识别、蛋白质功能的模拟等等。将这些任务做进一步的抽象，图分析任务可以大致分为以下四类：（a）节点分类，（b）链路预测，（c）聚类，以及（d）可视化\cite{goyal2018graph}。其中，节点分类旨在基于其他标记的节点和网络拓扑来确定节点的标签；链路预测是指预测缺失链路或未来可能出现的链路；聚类用于将相似节点聚合在一起；可视化有助于深入了解网络结构。

图由边和节点表示，如果直接使用这些关系表示图的信息，一般只能使用数学或统计的方式进行数据处理。如果将图转换到向量空间表示，则有更加丰富的方法可以对图进行处理。对于一般的应用，图使用 \(|V| \times |V|\) 的邻接矩阵表示，但这样的方法对于现实生活中的大型图结构进行表征几乎是不可能的。一方面使用邻接矩阵表示图的信息，空间复杂度超过企业可以接受的范围；另外一方面，这样的编码方式并不能很好地反映图节点的信息。因此，使用图嵌入技术将图中的节点以低维稠密向量的形式进行表示，实际上将图的信息进行了压缩，打包在一个维度更小的向量中。这要求图嵌入的技术对节点映射为一个低维向量时，尽可能地保留节点的拓扑信息，在原图中接近的节点在低维特征空间也同样比较接近。

\subsection{问题描述}

对于图 $G=(V,E)$，节点集合 $V=\{v_1,\dots,v_n\}$，边集合 $E=\{ e_{ij} \}_{i,j=1}^n$。图嵌入是图节点的映射，映射 $f$ 在将节点映射到低维特征向量的同时，尝试保留节点之间的拓扑信息。

$$
f:v_i \to y_i \in \mathbb{R}^d, \;\forall i \in[0,n],\;where\; d \ll |V|
$$

节点之间的拓扑信息有多种方法可以衡量，其中一种常用的方法为使用节点之间的距离表示节点之间的拓扑信息。即在原图中两个节点之间有邻接关系或者两个节点距离较近，在映射后在低维向量空间中对应两个向量的距离同样比较接近，在这里衡量两个向量距离的接近程度通常使用两个向量的内积。因此，当前节点对应的拓扑信息，可以使用其邻居节点进行表示。

% TODO: 可以用最大似然法再做一下公式化，反映给定向量的映射方式使通过向量能够重现节点邻居的概率最大

对于这一问题，DeepWalk 是其中一个最经典的方法，这个方法的思路和上述使用邻居节点表示节点的拓扑信息有异曲同工的地方。DeepWalk 使用的随机游走策略将在当前节点的邻居中等概率地选择下一个要访问节点，这实际上仅利用了节点之间的连接信息来提取节点序列，在一定程度上保留了节点的拓扑信息。虽然 DeepWalk 是 KDD 2014 的工作，但它为我们实现图嵌入提供了很好的思路，是我们了解图嵌入无法绕过的一个方法。

\subsection{本文工作}

本文主要讨论了图嵌入问题并深入了解 DeepWalk 算法的思路和实现，并且对 DeepWalk 进行一系列的讨论，尝试对不同类型的图改进 DeepWalk 中节点序列的提取的策略，根据节点以及连接之间更多的信息来指导节点的选择：

\begin{itemize}
    \item 根据采样节点与起始节点的距离，调整返回上一节点的概率。
    \item 根据采样节点的邻居的度数，调整选择该邻居作为下一节点的概率。
\end{itemize}

% TODO: 完成全篇再进行更新
本文的其余部分安排如下。在第二节中，我们将介绍图嵌入算法的发展。在第三节中，我们将讲述Deepwalk算法原理和改进。在第四节中，我们将展示和分析实验结果。在第五节中，我们将做出总结并给出未来的改进方向。

% TODO: 符号的相关表示。文案：文中用到的符号说明如下？\ref{notations}
\begin{table}[!t]
\renewcommand{\arraystretch}{1.1}
\caption{Important Notations}
\label{notations}
\centering
\begin{tabular}{|l|p{0.78\columnwidth}|}
    % TODO: 图的表示 G=(V, E)，强调无向图？最大似然的相关表示：概率、映射方式等
    % \hline
    % \(V\) & 当前节点的邻接节点对应的点集 \\
    \hline
    % FIXME: 描述可能不太准确
    \(u\) & DeepWalk 算法中随机游走讨论的当前节点 \\
    \hline
    \(V_u\) & 当前节点 \(u\) 对应邻接节点的点集 \\
    \hline
    \(v_i\) & 当前节点的邻接节点点集中下标为 \(i\) 的节点 \\
    \hline
    \(|n|\) & 节点 \(n\) 的度数，即 \(n\) 的邻接节点的个数 \\
    \hline
    % $b_n$ & Bandwidth of edge $n$ at time slot $t$ \\
    % \hline
    % $I$ & Idle period of hot start serverless function \\
    % \hline
    % $Q(t)$ & Number of cold start serverless function at time slot $t$ \\
    % \hline
    % $T_n^e$ & Computation time of edge $n$ \\
    % \hline
    % $T_m^s$ & Computation time of serverless function configuration $m$ \\
    % \hline
    % $T^c$ & Cold start time of serverless function container\\
    % \hline
    % $r(t)$ & Number of requests sent to cloud \\
    % \hline
    % $x_n(t)$ & Indicator of handling requests of edge $n$ at cloud at time $t$ \\
    % \hline
    % $y_{n,m}(t)$ & Indicator of handling requests of edge $n$ using serverless function configuration $m$ at time $t$ \\
    % \hline
\end{tabular}
\end{table}

\section{DeepWalk 方法的简单介绍}

\subsection{对图嵌入问题的进一步讨论}

图嵌入的核心目标在于将图上的节点映射到一个低维向量上，这一向量可以保留节点的拓扑信息。这个问题本质上是将一个高维稀疏的向量映射到一个低维稠密的向量上，并且能够保留相应的信息量。并且值得一提的是，这样的映射成立有其对应的原因，对于现实中的稀疏图，使用邻接矩阵存储网络的信息可能存在大量的信息冗余。一个节点的信息体主要体现在和它的邻居节点，以社交网络为例，其中存在两个常见的现象：

\begin{itemize}
    \item 社交平台上存在一些大 V，他们的影响力较大，和大量的节点存在邻接关系。
    \item 社交平台上存在不同的圈子：圈子内部的用户可能相互关联互相关注，甚至可能上述提到的大 V 就可能是圈子中的一个代表元；圈子之间的用户可能关联较少，甚至可能存在相互独立的子图。
\end{itemize}

上述提到的两个现象，尤其是圈子现象说明了使用邻接矩阵表示节点之间的关系可能存在大量的数据冗余。特别是两个相互独立的圈子（在一些关联较弱的圈子内部也可以近似看作两个相互独立的圈子）之间的用户代表两个相互独立的子图，在邻接矩阵中占据了冗余的信息空间。因此，使用节点的邻居节点的信息可以极大程度上表示节点的拓扑信息，在社交网络中可以对应为用户的圈子信息。

而且在上述的社交网络模型中，使用向量的内积作为衡量两个向量相似的指标，可以很好地解释上面的两个现象。通常而言，两个向量的夹角较小可以看作两个用户在同一个圈子内部；而向量的模比较大可以看作用户的影响力比较大，在其方向上和多个向量的内积都比较大，容易和多个用户产生关联。

综合上述两点，根据节点的邻接信息构建向量，进而描述图的相关信息有其的合理性；并且使用向量的内积作为衡量两个向量的相似程度，也十分具有解释性。

\subsection{DeepWalk 方法的介绍}

DeepWalk 使用了类似的思想使用节点的邻居信息表示节点的拓扑信息。通过随机游走的方式对邻居节点进行采样获得对应的节点的拓扑信息。DeepWalk 创造性地将词嵌入方法 word2vec 引入到图嵌入中，结合上述的邻居节点信息得到节点和节点的共现关系，再用 word2vec 模型将节点的邻接信息看作词句，再学习使用将上述的邻居信息编码为向量形式。

% TODO: 随机游走成立的原因（采样结果的分布，返回上一个节点的概率 etc
% TODO: 不同超参对结果的影响：随机游走序列长度（维度）的影响，不同数据集可能的影响（过拟合、欠拟合

% TODO: 使用 word2vec 和 NLP 的相似之处，幂律分布？

\newpage

\section{基于 DeepWalk 算法的进一步讨论}

在上面的讨论中可以得知，DeepWalk 使用随机游走的方式对邻居节点进行采样，实现的效果为距离当前节点较近的邻居节点被采样的概率比较大，距离当前节点比较远的邻居节点被采样的概率比较小。根据这一特性，我们可以对 DeepWalk 的采样策略进行详细的讨论，对不同类型的图作出进一步的讨论，依据不同类型的图结构，选择合适的采样方法使得 DeepWalk 的准确率尽可能提升。从采样策略和图的性质出发，我们提出了两个角度对采样策略进行讨论：基于图节点度数的改进方式、基于路径的改进方式。

\subsection{基于度数对 DeepWalk 的讨论}

考虑图的度数对 DeepWalk 采样策略的影响，由于随机游走策略是从当前节点随机选择一个邻接节点进行移动，然后在该邻接节点上继续迭代算法，我们可以得知不同的节点度数对 DeepWalk 的采样行为造成不同的结果。我们可以考虑如下场景：程序进行随机游走，如果新到达的节点的度数比较大，则程序随机游走选择往回行走，即前往上一个节点的概率比较小；相反如果新到达的节点度数比较大，则程序随机游走选择往回行走的概率比较小。

根据上述的性质，我们可以设计不同的采样策略对度数较大的邻接节点和度数较小的邻接节点进行一定的权衡比较。我们可以依据这一想法，设计出依据当前节点的邻接节点度数对选择下一个游走节点的概率进行调整的两种方法：优先选择度数更大的邻接节点、优先选择度数更小的邻接节点。

% TODO: 适用的图和意义不知道放哪里比较合适

设计优先选择度数较大的邻接节点，可以很容易可以想到实现的思路。我们可以直接依据不同节点的度数占比作为选择节点的概率，可以使用以下公式进行表述，其中 \(p(v_i)\) 为选择节点 \(v_i\) 作为随机游走的目标点的概率：

$$
p(v_i) = \frac{|v_i|}{\sum_{j=1}^{|u|} |v_j|}, \;\forall v_i, v_j \in V_u
$$

设计优先选择度数较小的邻接节点，有多种概率映射的方式，只要最终达到概率和邻接节点的度数呈负相关即可。我们依据取相反数和取倒数两个角度，提出了以下三种映射方式：

以相反数的角度出发，我们可以使用常数 \(C\) 减度数占比的方式再进行归一化，求出各个邻接节点被选择的概率。以公式化的语言进行描述，可以得到以下表达式：

$$
p(v_i) = \left(C - \frac{|v_i|}{\sum_{j=1}^{|u|} |v_j|}\right) / \left(C|u| - 1\right), \;\forall v_i, v_j \in V_u, \;C > 1
$$

在具体实验中我们取 \(C = 2\)而不取 \(C = 1\)，避免由于邻接节点只有一个邻居，即 \(|u| = 1\) 的情况，导致概率中分子项为 0 的情况，具体的概率表达式为：

$$
p(v_i) = \left(2 - \frac{|v_i|}{\sum_{j=1}^{|u|} |v_j|}\right) / \left(2|u| - 1\right), \;\forall v_i, v_j \in V_u
$$

当然，我们也可以取 \(C = 1\) 的情况，在这种情况下需要我们对分子为 0 的这一情况进行特判，具体的概率表达式为：

\begin{equation}
p(v_i) = \begin{cases}
    \left ( 1 - \frac{|v_i|}{\sum_{j=1}^{|u|} |v_j|} \right ) / (|u| - 1),& |u| > 1 \\
    1,& |u| = 1 \\
\end{cases}
\end{equation}

从倒数的角度出发，我们可以将邻接节点的度数映射为 \(\hat{p}(v_i) = \frac{\sum_{j=1}^{|u|} |v_j|}{|v_i|}\)，这样同样可以达到选取的概率随着节点度数的增加而减少。因此，我们可以设计出以下的概率映射关系：

$$
p(v_i) = \frac{\hat{p}(v_i)}{\sum_{k=1}^{|u|} \hat{p}(v_k)}, \;\text{where} \;\hat{p}(v_i) = \frac{\sum_{j=1}^{|u|} |v_j|}{|v_i|}
$$

\subsection{基于度数讨论的结果分析}

我们在上节通过度数对 DeepWalk 的采样方法进行了一系列的讨论，主要提出了依据邻接节点的度数选择不同的方案。我们猜想这两种方法在不同类型上的图会有各自擅长的表现。例如对一些大型社交平台，优先选择度数较大的邻接节点，可能会有更好的表现。依据我们的猜想，在大型的社交平台中人们更多的是表现自己的主要属性，在这样的网络上人们更多的是关注自己的感兴趣领域的一些大 V。因此，通过这些度数比较大的邻接节点可以更好地表现节点的信息。相反，对于一些小型的社交平台，特别是一些亚文化的社交平台，人们关注的更多是一些更具个性化、更具特色的圈子。这样的圈子可能没有明显的大 V，更多的是用户之间圈地自萌，主要体现在用户和用户之间相互关注，一般并不会主动跨圈关注其他大 V。因此，对于一些小型社交平台更适合使用度数较小的邻接节点作为采样对象。

为了验证我们的想法，我们使用了三个数据集对原方法和变体的方法进行了比较。每个方法在每一个数据集上运行了十次，并且最后求出平均值。得到的结果如表 \ref{exp: deg result}所示：

通过表格可以看出在机场数据集中，无论是 usa-airport 还是 europe-airport 数据集，优先选择节点度数大的方法效果比原本的方法要好，优先选择节点度数小的方法得到的效果比原方法效果要差。我们可以使用上述的猜想进行解释。在机场数据集下，机场具有的标签更多是从属于比较大的机场，即小机场的飞机通常多飞往大机场，这个时候可以视作小机场在大机场的辐射范围内。因此，可以使用大机场的标签作为小机场的标签。因此，使用节点度数较大的邻接节点作为采样样本，在机场数据集中更加合理。

对于另一个 wiki 数据集，通过比较可以得知：优先选择节点度数小的方法效果比原本的随机采样要好，并且这样的优势在三种映射方式上都有所体现，而使用节点度数大的方法得到的效果比原方法的效果更差。使用我们前面提到的猜想同样可以对结果进行解释：wiki 上面的词条相互引用连接形成图，要想比较好得反映出词条的属性，使用的是和其关联的同一领域的相关词条，而不是引用量高的词条。这样的词条往往没有被很多的词条所引用，只是领域内的相关词条引用，因此这样的词条度数一般较少，但却能更好的反映词条的属性。

\begin{table}[!t]
\renewcommand{\arraystretch}{1.1}
\caption{基于度数的实验结果}
\label{exp: deg result}
\centering
\begin{tabular}{|l|l|l|l|l|l|} % p{0.3\columnwidth}|}
    \hline
    数据集 & 随机游走 & 大度数优先 & 小度数优先法1 & 小度数优先法2 & 小度数优先法3 \\
    \hline
    wiki \\
    \hline
    usa airport \\
    \hline
    europe airport \\
    \hline
\end{tabular}
\end{table}

\subsection{基于路径对 DeepWalk 的讨论}

从随机游走的方法出发，我们还可以对随机游走的路径长度进行分析。考虑随机游走的原理，作者使用随机游走的原因在于这样的行走本质上表现为一种采样，采样的概率密度函数分布随着和当前节点的距离拉开而逐渐减小。因此，实现了更多使用距离节点比较近的邻居节点表示当前节点的信息，而距离当前节点比较远的邻居节点被采样的概率减小，从而避免当前节点的属性存在过多偏差。

从这一角度出发，我们设计出一种新的采样方式，可以人为增强这种约束：随着采样路径的增长，我们适当增加往回行走的采样策略。

\subsection{基于路径讨论的结果分析}

\section{相关工作陈述}

图嵌入算法是一种将图中节点、边及其特征转换为较低维度的向量空间，同时最大限度地保留图结构和信息等属性的方法。约二十年前，人们提出了图嵌入算法，算法思想是根据实际问题构造一个D维空间中的图，然后将图的节点嵌入到d（d<<D）维向量空间中，嵌入指的是在向量空间中保持邻接的节点彼此靠近。在过去的十年里，在图嵌入领域已经有了大量的研究，重点是设计新的嵌入算法。发展到现在，大体上可以将这些嵌入方法分为三大类：（1）基于因子分解的方法，（2）基于随机游走的方法，以及（3）基于深度学习的方法\cite{goyal2018graph}。

\subsection{基于因子分解的方法}

\begin{itemize}
    \item Locally Linear Embedding，局部线性嵌入，局部线性假设每个节点都是相邻节点的线性组合。通过最小化目标函数$\phi(Y)=\sum_{i=1}^N|y_i-\sum_{j=1}^kw_{ij}y_j|^2$得到嵌入矩阵$Y$。
    \item Laplacian Eigenmaps，拉普拉斯特征映射，在权重$w_{ij}$较高时，如果在嵌入后被分割过远，则给予更高的惩罚。通过最小化目标函数$\phi(Y)=\frac{1}{2}\sum_{ij} |y_i-y_j|^2w_{ij}$进行求解。
\end{itemize}

\subsection{基于随机游走的方法}

\begin{itemize}
    \item DeepWalk受到word2vec的启发，选择某一特定点作为起始点，通过随机游走得到一系列点的序列，用word2vec来学习该序列，从而得到表示该点的向量。
    \item node2vec和DeepWalk类似，但在随机游走这一步骤中使用有偏随机游走，即通过参数控制，使得随机游走反映出类似于DFS或BFS的采样特性，从而提高采样的效果。
\end{itemize}

\subsection{基于深度学习的方法}
\begin{itemize}
    \item SDNE使用深度自动编码器来保持一阶和二阶网络临近度，该方法通过自动编码器来寻找一个重构邻域节点的嵌入，并基于拉普拉斯特征映射来对嵌入结果进行评判反馈。
    \item DNGR使用随机游走模型来生成概率共现矩阵，将该矩阵转化为PPMI矩阵，输入到叠加去噪自动编码器中得到嵌入结果。
\end{itemize}

\section{总结与展望}

\bibliography{reference}
\end{document}

% \begin{figure}
%     \centering]
%     \includegraphics[width=8cm]{image/exec-overview.png}
%     \caption{MapReduce: Execution overview}
%     \label{fig: exec-overview}
% \end{figure}
